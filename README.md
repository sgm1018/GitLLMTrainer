# GitHub Repository Crawler

![Project Banner](https://github.com/user-attachments/assets/d619ab42-6544-4252-8d65-a9547dde9e61)

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10.6%2B-blue?logo=python&logoColor=white" alt="Python Badge">
</p>

## Overview
The **GitHub Repository Crawler** is a Python-based tool that enables users to scrape the contents of public GitHub repositories. It recursively fetches files, directories, and their raw contents, saving the data in a structured JSON format. The tool is especially useful for developers, researchers, and analysts seeking to analyze repository content programmatically.

## Features
- **Recursively Scrape Repositories**: Extract all files and folders from a given repository.
- **Fetch Raw File Content**: Retrieve and store the raw content of files.
- **Structured JSON Output**: Save the scraped data in an organized and human-readable JSON format.

## Table of Contents
1. [Getting Started](#getting-started)
2. [Project Structure](#project-structure)
3. [Installation](#installation)
4. [Usage](#usage)
5. [Examples](#examples)
6. [Contributing](#contributing)
7. [License](#license)

## Getting Started
Follow the steps below to set up and use the GitHub Repository Crawler on your local machine.

### Prerequisites
- Python 3.10.6 or +
- GitHub API token (optional, for private repositories and to avoid rate limit(you have more requests))

### Project Structure
```
repo/
├── src/
│   ├── crawler/
│   │   ├── github_api.py
│   │   ├── json_formatter.py
│   │   ├── logger.py
│   │   ├── scraper.py
│   └── utils/
│       ├── helpers.py
├── tests/
│   ├── test_github_api.py
│   ├── test_scrapper.py
├── requirements.txt
├── setup.py
└── main.py
```

## Installation
### Step 1: Clone the Repository
```bash
git clone https://github.com/<your-username>/<your-repo>.git
cd <your-repo>
```

### Step 2: Set Up a Virtual Environment
```bash
python -m venv venv
source venv/bin/activate # On Windows use `venv\Scripts\activate`
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

## Usage
### Run the Tool
```bash
python src/main.py <GitHub repository URL>
```
### Parameters
- `<GitHub repository URL>`: URL of the GitHub repository to scrape (e.g., `https://github.com/sgm1018/GitLLMTrainer`).

### Example Output
- The data will be saved in `repo_data.json`.
```json
[
  {
    "path": "src/crawler/logger.py",
    "type": "file",
    "content": "import logging\nfrom colorlog ..."
  },
  ...
]
```

## Examples
To scrape the contents of the `tldraw/tldraw` repository:
```bash
python src/main.py https://github.com/tldraw/tldraw
```
Expected output:
```
INFO: Starting scraping for https://github.com/sgm1018/GitLLMTrainer
INFO: File scraped: /README.md
INFO: Entering directory: /src
...
INFO: Scraping completed for https://github.com/sgm1018/GitLLMTrainer
INFO: Data saved to repo_data.json
```

## Advanced Use Cases
The structured JSON output generated by this tool can be utilized in advanced scenarios such as:

- **Training Custom Models**: Use the JSON data to fine-tune custom OpenAI models by converting repository content into meaningful datasets.
- **Knowledge Integration for Private Models**: The extracted data can serve as a knowledge base for private AI models, enabling personalized applications for your projects.

## Best Practices
- Use a GitHub API token to avoid rate limits.
- Run the tool in a virtual environment.

## Contributing
Contributions are welcome!

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

