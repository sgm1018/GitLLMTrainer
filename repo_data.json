[
    {
        "path": "/.gitignore",
        "type": "file",
        "content": "venv\r\nsrc/config.py"
    },
    {
        "path": "/README.md",
        "type": "file",
        "content": ""
    },
    {
        "path": "/requirements.txt",
        "type": "file",
        "content": "requests\r\ncolorlog\r\ntqdm"
    },
    {
        "path": "/setup.py",
        "type": "file",
        "content": "\r\n"
    },
    {
        "path": "/src/__init__.py",
        "type": "file",
        "content": ""
    },
    {
        "path": "/src/__pycache__/config.cpython-310.pyc",
        "type": "file",
        "content": "o\r\r\n\u0000\u0000\u0000\u0000[�zg}\u0000\u0000\u0000�\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000@\u0000\u0000\u0000s\f\u0000\u0000\u0000d\u0000Z\u0000d\u0001Z\u0001d\u0001S\u0000)\u0002z\u0016https://api.github.comN)\u0002�\u000eGITHUB_API_URLZ\fGITHUB_TOKEN�\u0000r\u0002\u0000\u0000\u0000r\u0002\u0000\u0000\u0000�%D:\\Proyectos\\gitCrawler\\src\\config.py�\b<module>\u0001\u0000\u0000\u0000s\u0004\u0000\u0000\u0000\u0004\u0000\b\u0001"
    },
    {
        "path": "/src/config.py",
        "type": "file",
        "content": "GITHUB_API_URL = \"https://api.github.com\"\r\nGITHUB_TOKEN = None  # Reemplaza con tu token si deseas acceder a repos privados\r\n"
    },
    {
        "path": "/src/crawler/__init__.py",
        "type": "file",
        "content": ""
    },
    {
        "path": "/src/crawler/__pycache__/__init__.cpython-310.pyc",
        "type": "file",
        "content": "o\r\r\n\u0000\u0000\u0000\u0000V�zg\u0000\u0000\u0000\u0000�\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000@\u0000\u0000\u0000s\u0004\u0000\u0000\u0000d\u0000S\u0000)\u0001N�\u0000r\u0001\u0000\u0000\u0000r\u0001\u0000\u0000\u0000r\u0001\u0000\u0000\u0000�/D:\\Proyectos\\gitCrawler\\src\\crawler\\__init__.py�\b<module>\u0001\u0000\u0000\u0000s\u0002\u0000\u0000\u0000\u0004\u0000"
    },
    {
        "path": "/src/crawler/__pycache__/github_api.cpython-310.pyc",
        "type": "file",
        "content": "o\r\r\n\u0000\u0000\u0000\u0000��zg�\u0002\u0000\u0000�\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0003\u0000\u0000\u0000@\u0000\u0000\u0000s>\u0000\u0000\u0000d\u0000d\u0001l\u0000Z\u0000d\u0000d\u0001l\u0001Z\u0001e\u0001j\u0002�\u0003d\u0002�\u0001\u0001\u0000d\u0000d\u0003l\u0004m\u0005Z\u0005m\u0006Z\u0006\u0001\u0000G\u0000d\u0004d\u0005�\u0000d\u0005�\u0002Z\u0007d\u0001S\u0000)\u0006�\u0000\u0000\u0000\u0000Nz\u0002..)\u0002�\u000eGITHUB_API_URL�\fGITHUB_TOKENc\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0002\u0000\u0000\u0000@\u0000\u0000\u0000s$\u0000\u0000\u0000e\u0000Z\u0001d\u0000Z\u0002d\u0001d\u0002�\u0000Z\u0003d\u0003d\u0004�\u0000Z\u0004d\u0005d\u0006�\u0000Z\u0005d\u0007S\u0000)\b�\tGitHubAPIc\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0003\u0000\u0000\u0000C\u0000\u0000\u0000s\"\u0000\u0000\u0000t\u0000r\fd\u0001d\u0002t\u0000�\u0000�\u0002i\u0001|\u0000_\u0001d\u0000S\u0000i\u0000|\u0000_\u0001d\u0000S\u0000)\u0003N�\rAuthorizationz\u0006token )\u0002r\u0003\u0000\u0000\u0000�\u0007headers)\u0001�\u0004self�\u0000r\b\u0000\u0000\u0000�1D:\\Proyectos\\gitCrawler\\src\\crawler\\github_api.py�\b__init__\b\u0000\u0000\u0000s\u0002\u0000\u0000\u0000\"\u0001z\u0012GitHubAPI.__init__c\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0003\u0000\u0000\u0000\u0004\u0000\u0000\u0000C\u0000\u0000\u0000s \u0000\u0000\u0000t\u0000j\u0001|\u0001|\u0000j\u0002d\u0001�\u0002}\u0002|\u0002�\u0003�\u0000\u0001\u0000|\u0002�\u0004�\u0000S\u0000)\u0002z6Fetch the contents of a directory from the GitHub API.�\u0001r\u0006\u0000\u0000\u0000)\u0005�\brequests�\u0003getr\u0006\u0000\u0000\u0000�\u0010raise_for_status�\u0004json)\u0003r\u0007\u0000\u0000\u0000�\u0003url�\bresponser\b\u0000\u0000\u0000r\b\u0000\u0000\u0000r\t\u0000\u0000\u0000�\u000ffetch_directory\u000b\u0000\u0000\u0000s\u0006\u0000\u0000\u0000\u0010\u0002\b\u0001\b\u0001z\u0019GitHubAPI.fetch_directoryc\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0003\u0000\u0000\u0000\u0004\u0000\u0000\u0000C\u0000\u0000\u0000s\u001e\u0000\u0000\u0000t\u0000j\u0001|\u0001|\u0000j\u0002d\u0001�\u0002}\u0002|\u0002�\u0003�\u0000\u0001\u0000|\u0002j\u0004S\u0000)\u0002z Fetch the raw content of a file.r\u000b\u0000\u0000\u0000)\u0005r\f\u0000\u0000\u0000r\r\u0000\u0000\u0000r\u0006\u0000\u0000\u0000r\u000e\u0000\u0000\u0000�\u0004text)\u0003r\u0007\u0000\u0000\u0000�\fdownload_urlr\u0011\u0000\u0000\u0000r\b\u0000\u0000\u0000r\b\u0000\u0000\u0000r\t\u0000\u0000\u0000�\u0012fetch_file_content\u0011\u0000\u0000\u0000s\u0006\u0000\u0000\u0000\u0010\u0002\b\u0001\u0006\u0001z\u001cGitHubAPI.fetch_file_contentN)\u0006�\b__name__�\n__module__�\f__qualname__r\n\u0000\u0000\u0000r\u0012\u0000\u0000\u0000r\u0015\u0000\u0000\u0000r\b\u0000\u0000\u0000r\b\u0000\u0000\u0000r\b\u0000\u0000\u0000r\t\u0000\u0000\u0000r\u0004\u0000\u0000\u0000\u0007\u0000\u0000\u0000s\b\u0000\u0000\u0000\b\u0000\b\u0001\b\u0003\f\u0006r\u0004\u0000\u0000\u0000)\br\f\u0000\u0000\u0000�\u0003sys�\u0004path�\u0006append�\u0006configr\u0002\u0000\u0000\u0000r\u0003\u0000\u0000\u0000r\u0004\u0000\u0000\u0000r\b\u0000\u0000\u0000r\b\u0000\u0000\u0000r\b\u0000\u0000\u0000r\t\u0000\u0000\u0000�\b<module>\u0001\u0000\u0000\u0000s\n\u0000\u0000\u0000\b\u0000\b\u0001\f\u0001\u0010\u0001\u0012\u0003"
    },
    {
        "path": "/src/crawler/__pycache__/json_formatter.cpython-310.pyc",
        "type": "file",
        "content": "o\r\r\n\u0000\u0000\u0000\u0000��zg\b\u0001\u0000\u0000�\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0003\u0000\u0000\u0000@\u0000\u0000\u0000s\u001a\u0000\u0000\u0000d\u0000d\u0001l\u0000Z\u0000G\u0000d\u0002d\u0003�\u0000d\u0003�\u0002Z\u0001d\u0001S\u0000)\u0004�\u0000\u0000\u0000\u0000Nc\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0003\u0000\u0000\u0000@\u0000\u0000\u0000s\u0018\u0000\u0000\u0000e\u0000Z\u0001d\u0000Z\u0002e\u0003d\u0001d\u0002�\u0000�\u0001Z\u0004d\u0003S\u0000)\u0004�\rJSONFormatterc\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0003\u0000\u0000\u0000\b\u0000\u0000\u0000C\u0000\u0000\u0000sF\u0000\u0000\u0000t\u0000|\u0001d\u0001d\u0002d\u0003�\u0003�\u0012}\u0002t\u0001j\u0002|\u0000|\u0002d\u0004d\u0005d\u0006�\u0004\u0001\u0000W\u0000d\u0007\u0004\u0000\u0004\u0000�\u0003\u0001\u0000d\u0007S\u00001\u0000s\u001cw\u0001\u0001\u0000\u0001\u0000\u0001\u0000Y\u0000\u0001\u0000d\u0007S\u0000)\bz\u0019Save data to a JSON file.�\u0001wz\u0005utf-8)\u0001�\bencoding�\u0004\u0000\u0000\u0000F)\u0002�\u0006indent�\fensure_asciiN)\u0003�\u0004open�\u0004json�\u0004dump)\u0003�\u0004dataZ\u000boutput_path�\u0001f�\u0000r\r\u0000\u0000\u0000�5D:\\Proyectos\\gitCrawler\\src\\crawler\\json_formatter.py�\fsave_to_file\u0005\u0000\u0000\u0000s\u0006\u0000\u0000\u0000\u0010\u0003\u0014\u0001\"�z\u001aJSONFormatter.save_to_fileN)\u0005�\b__name__�\n__module__�\f__qualname__�\fstaticmethodr\u000f\u0000\u0000\u0000r\r\u0000\u0000\u0000r\r\u0000\u0000\u0000r\r\u0000\u0000\u0000r\u000e\u0000\u0000\u0000r\u0002\u0000\u0000\u0000\u0004\u0000\u0000\u0000s\u0006\u0000\u0000\u0000\b\u0000\u0002\u0001\u000e\u0001r\u0002\u0000\u0000\u0000)\u0002r\t\u0000\u0000\u0000r\u0002\u0000\u0000\u0000r\r\u0000\u0000\u0000r\r\u0000\u0000\u0000r\r\u0000\u0000\u0000r\u000e\u0000\u0000\u0000�\b<module>\u0001\u0000\u0000\u0000s\u0004\u0000\u0000\u0000\b\u0000\u0012\u0003"
    },
    {
        "path": "/src/crawler/__pycache__/logger.cpython-310.pyc",
        "type": "file",
        "content": "o\r\r\n\u0000\u0000\u0000\u0000��zg/\u0002\u0000\u0000�\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0002\u0000\u0000\u0000@\u0000\u0000\u0000s \u0000\u0000\u0000d\u0000d\u0001l\u0000Z\u0000d\u0000d\u0002l\u0001m\u0002Z\u0002\u0001\u0000d\u0003d\u0004�\u0000Z\u0003d\u0001S\u0000)\u0005�\u0000\u0000\u0000\u0000N)\u0001�\u0010ColoredFormatterc\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0002\u0000\u0000\u0000\t\u0000\u0000\u0000C\u0000\u0000\u0000sH\u0000\u0000\u0000t\u0000�\u0001d\u0001�\u0001}\u0000|\u0000�\u0002t\u0000j\u0003�\u0001\u0001\u0000t\u0000�\u0004�\u0000}\u0001|\u0001�\u0005t\u0006d\u0002d\u0003d\u0004d\u0005d\u0006d\u0007�\u0004d\b�\u0002�\u0001\u0001\u0000|\u0000�\u0007|\u0001�\u0001\u0001\u0000|\u0000S\u0000)\tz$Create a logger with colored output.�\rGitHubScraperz(%(log_color)s[%(levelname)s] %(message)sZ\u0004cyanZ\u0006yellow�\u0003red�\u0004blue)\u0004�\u0004INFO�\u0007WARNING�\u0005ERROR�\u0005DEBUG)\u0001Z\nlog_colors)\b�\u0007logging�\tgetLogger�\bsetLevelr\u0006\u0000\u0000\u0000�\rStreamHandler�\fsetFormatterr\u0002\u0000\u0000\u0000�\naddHandler)\u0002�\u0006logger�\u0007handler�\u0000r\u0012\u0000\u0000\u0000�-D:\\Proyectos\\gitCrawler\\src\\crawler\\logger.py�\nget_logger\u0005\u0000\u0000\u0000s\u001a\u0000\u0000\u0000\n\u0002\f\u0001\b\u0001\u0006\u0001\u0002\u0001\u0002\u0002\u0002\u0001\u0002\u0001\u0002\u0001\u0004�\b�\n\t\u0004\u0001r\u0014\u0000\u0000\u0000)\u0004r\n\u0000\u0000\u0000Z\bcolorlogr\u0002\u0000\u0000\u0000r\u0014\u0000\u0000\u0000r\u0012\u0000\u0000\u0000r\u0012\u0000\u0000\u0000r\u0012\u0000\u0000\u0000r\u0013\u0000\u0000\u0000�\b<module>\u0001\u0000\u0000\u0000s\u0006\u0000\u0000\u0000\b\u0000\f\u0001\f\u0003"
    },
    {
        "path": "/src/crawler/__pycache__/scraper.cpython-310.pyc",
        "type": "file",
        "content": "o\r\r\n\u0000\u0000\u0000\u0000��zg:\u0007\u0000\u0000�\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0003\u0000\u0000\u0000@\u0000\u0000\u0000sN\u0000\u0000\u0000d\u0000d\u0001l\u0000m\u0001Z\u0001\u0001\u0000d\u0000d\u0002l\u0002m\u0003Z\u0003\u0001\u0000d\u0000d\u0003l\u0004m\u0005Z\u0005\u0001\u0000d\u0000d\u0004l\u0006m\u0007Z\u0007\u0001\u0000d\u0000d\u0005l\bm\bZ\b\u0001\u0000G\u0000d\u0006d\u0007�\u0000d\u0007�\u0002Z\td\bS\u0000)\t�\u0000\u0000\u0000\u0000)\u0001�\u000eGITHUB_API_URL)\u0001�\tGitHubAPI)\u0001�\nget_logger)\u0001�\rJSONFormatter)\u0001�\u0004tqdmc\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0002\u0000\u0000\u0000@\u0000\u0000\u0000s,\u0000\u0000\u0000e\u0000Z\u0001d\u0000Z\u0002d\u0001d\u0002�\u0000Z\u0003d\u0003d\u0004�\u0000Z\u0004d\u0005d\u0006�\u0000Z\u0005d\u0007d\b�\u0000Z\u0006d\tS\u0000)\n�\rGitHubScraperc\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0002\u0000\u0000\u0000\u0003\u0000\u0000\u0000C\u0000\u0000\u0000s,\u0000\u0000\u0000t\u0000�\u0000|\u0000_\u0001|\u0001|\u0000_\u0002|\u0000�\u0003|\u0001�\u0001|\u0000_\u0004g\u0000|\u0000_\u0005t\u0006�\u0000|\u0000_\u0007d\u0000S\u0000)\u0001N)\br\u0003\u0000\u0000\u0000�\u0003api�\brepo_url�\u0013_convert_to_api_url�\frepo_api_url�\tdata_treer\u0004\u0000\u0000\u0000�\u0006logger)\u0002�\u0004selfr\t\u0000\u0000\u0000�\u0000r\u000f\u0000\u0000\u0000�.D:\\Proyectos\\gitCrawler\\src\\crawler\\scraper.py�\b__init__\t\u0000\u0000\u0000s\n\u0000\u0000\u0000\b\u0001\u0006\u0001\f\u0001\u0006\u0001\f\u0001z\u0016GitHubScraper.__init__c\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0005\u0000\u0000\u0000\u0006\u0000\u0000\u0000C\u0000\u0000\u0000s8\u0000\u0000\u0000|\u0001�\u0000d\u0001�\u0001�\u0001d\u0001�\u0001}\u0002|\u0002d\u0002\u0019\u0000|\u0002d\u0003\u0019\u0000\u0002\u0002}\u0003}\u0004t\u0002�\u0000d\u0004|\u0003�\u0000d\u0001|\u0004�\u0000d\u0005�\u0006S\u0000)\u0006z6Convert a GitHub repository URL to the API equivalent.�\u0001/����������z\u0007/repos/z\t/contents)\u0003�\u0006rstrip�\u0005splitr\u0002\u0000\u0000\u0000)\u0005r\u000e\u0000\u0000\u0000r\t\u0000\u0000\u0000�\u0005parts�\u0005ownerZ\u0004repor\u000f\u0000\u0000\u0000r\u000f\u0000\u0000\u0000r\u0010\u0000\u0000\u0000r\n\u0000\u0000\u0000\u0010\u0000\u0000\u0000s\u0006\u0000\u0000\u0000\u0010\u0002\u0012\u0001\u0016\u0001z!GitHubScraper._convert_to_api_urlc\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0004\u0000\u0000\u0000C\u0000\u0000\u0000s<\u0000\u0000\u0000|\u0000j\u0000�\u0001d\u0001|\u0000j\u0002�\u0000�\u0002�\u0001\u0001\u0000|\u0000�\u0003|\u0000j\u0004d\u0002�\u0002\u0001\u0000|\u0000j\u0000�\u0001d\u0003|\u0000j\u0002�\u0000�\u0002�\u0001\u0001\u0000|\u0000j\u0005S\u0000)\u0004z%Start scraping the GitHub repository.z\u0016Starting scraping for �\u0000z\u0017Scraping completed for )\u0006r\r\u0000\u0000\u0000�\u0004infor\t\u0000\u0000\u0000�\u0011_scrape_directoryr\u000b\u0000\u0000\u0000r\f\u0000\u0000\u0000)\u0001r\u000e\u0000\u0000\u0000r\u000f\u0000\u0000\u0000r\u000f\u0000\u0000\u0000r\u0010\u0000\u0000\u0000�\u0006scrape\u0016\u0000\u0000\u0000s\b\u0000\u0000\u0000\u0014\u0002\u000e\u0001\u0014\u0001\u0006\u0001z\u0014GitHubScraper.scrapec\u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0006\u0000\u0000\u0000\b\u0000\u0000\u0000C\u0000\u0000\u0000s�\u0000\u0000\u0000|\u0000j\u0000�\u0001|\u0001�\u0001}\u0003t\u0002|\u0003d\u0001|\u0002�\u0000�\u0002d\u0002d\u0003�\u0003D\u0000]M}\u0004|\u0004d\u0004\u0019\u0000d\u0002k\u0002r=|\u0000j\u0000�\u0003|\u0004d\u0005\u0019\u0000�\u0001}\u0005|\u0000j\u0004�\u0005|\u0002�\u0000d\u0006|\u0004d\u0007\u0019\u0000�\u0000�\u0003d\u0002|\u0005d\b�\u0003�\u0001\u0001\u0000|\u0000j\u0006�\u0007d\t|\u0004d\n\u0019\u0000�\u0000�\u0002�\u0001\u0001\u0000q\u0010|\u0004d\u0004\u0019\u0000d\u000bk\u0002r]|\u0000j\u0006�\u0007d\f|\u0004d\n\u0019\u0000�\u0000�\u0002�\u0001\u0001\u0000|\u0000�\b|\u0004d\r\u0019\u0000|\u0002�\u0000d\u0006|\u0004d\u0007\u0019\u0000�\u0000�\u0003�\u0002\u0001\u0000q\u0010d\u000eS\u0000)\u000fz\u001fRecursively scrape a directory.z\u000bProcessing �\u0004file)\u0002�\u0004desc�\u0004unit�\u0004typeZ\fdownload_urlr\u0012\u0000\u0000\u0000�\u0004name)\u0003�\u0004pathr \u0000\u0000\u0000�\u0007contentz\u000eFile scraped: r\"\u0000\u0000\u0000�\u0003dirz\u0014Entering directory: �\u0003urlN)\tr\b\u0000\u0000\u0000Z\u000ffetch_directoryr\u0006\u0000\u0000\u0000Z\u0012fetch_file_contentr\f\u0000\u0000\u0000�\u0006appendr\r\u0000\u0000\u0000r\u001a\u0000\u0000\u0000r\u001b\u0000\u0000\u0000)\u0006r\u000e\u0000\u0000\u0000Z\u0007api_urlZ\u000bpath_prefix�\bcontents�\u0004itemr#\u0000\u0000\u0000r\u000f\u0000\u0000\u0000r\u000f\u0000\u0000\u0000r\u0010\u0000\u0000\u0000r\u001b\u0000\u0000\u0000\u001d\u0000\u0000\u0000s\u001e\u0000\u0000\u0000\f\u0002\u0018\u0001\f\u0001\u0010\u0001\u0006\u0001\u0010\u0001\u0002\u0001\u0002\u0001\b�\u0018\u0005\f\u0001\u0016\u0001\u001e\u0001\u0002�\u0004�z\u001fGitHubScraper._scrape_directoryN)\u0007�\b__name__�\n__module__�\f__qualname__r\u0011\u0000\u0000\u0000r\n\u0000\u0000\u0000r\u001c\u0000\u0000\u0000r\u001b\u0000\u0000\u0000r\u000f\u0000\u0000\u0000r\u000f\u0000\u0000\u0000r\u000f\u0000\u0000\u0000r\u0010\u0000\u0000\u0000r\u0007\u0000\u0000\u0000\b\u0000\u0000\u0000s\n\u0000\u0000\u0000\b\u0000\b\u0001\b\u0007\b\u0006\f\u0007r\u0007\u0000\u0000\u0000N)\n�\u0006configr\u0002\u0000\u0000\u0000Z\u0012crawler.github_apir\u0003\u0000\u0000\u0000�\u000ecrawler.loggerr\u0004\u0000\u0000\u0000�\u0016crawler.json_formatterr\u0005\u0000\u0000\u0000r\u0006\u0000\u0000\u0000r\u0007\u0000\u0000\u0000r\u000f\u0000\u0000\u0000r\u000f\u0000\u0000\u0000r\u000f\u0000\u0000\u0000r\u0010\u0000\u0000\u0000�\b<module>\u0001\u0000\u0000\u0000s\f\u0000\u0000\u0000\f\u0000\f\u0001\f\u0001\f\u0001\f\u0001\u0012\u0003"
    },
    {
        "path": "/src/crawler/github_api.py",
        "type": "file",
        "content": "import requests\r\nimport sys\r\nsys.path.append('..')\r\nfrom config import GITHUB_API_URL, GITHUB_TOKEN\r\n\r\n\r\nclass GitHubAPI:\r\n    def __init__(self):\r\n        self.headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"} if GITHUB_TOKEN else {}\r\n\r\n    def fetch_directory(self, url):\r\n        \"\"\"Fetch the contents of a directory from the GitHub API.\"\"\"\r\n        response = requests.get(url, headers=self.headers)\r\n        response.raise_for_status()\r\n        return response.json()\r\n\r\n    def fetch_file_content(self, download_url):\r\n        \"\"\"Fetch the raw content of a file.\"\"\"\r\n        response = requests.get(download_url, headers=self.headers)\r\n        response.raise_for_status()\r\n        return response.text\r\n"
    },
    {
        "path": "/src/crawler/json_formatter.py",
        "type": "file",
        "content": "import json\r\n\r\n\r\nclass JSONFormatter:\r\n    @staticmethod\r\n    def save_to_file(data, output_path):\r\n        \"\"\"Save data to a JSON file.\"\"\"\r\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\r\n            json.dump(data, f, indent=4, ensure_ascii=False)\r\n"
    },
    {
        "path": "/src/crawler/logger.py",
        "type": "file",
        "content": "import logging\r\nfrom colorlog import ColoredFormatter\r\n\r\n\r\ndef get_logger():\r\n    \"\"\"Create a logger with colored output.\"\"\"\r\n    logger = logging.getLogger(\"GitHubScraper\")\r\n    logger.setLevel(logging.INFO)\r\n    handler = logging.StreamHandler()\r\n    handler.setFormatter(ColoredFormatter(\r\n        \"%(log_color)s[%(levelname)s] %(message)s\",\r\n        log_colors={\r\n            \"INFO\": \"cyan\",\r\n            \"WARNING\": \"yellow\",\r\n            \"ERROR\": \"red\",\r\n            \"DEBUG\": \"blue\"\r\n        }\r\n    ))\r\n    logger.addHandler(handler)\r\n    return logger\r\n"
    },
    {
        "path": "/src/crawler/scraper.py",
        "type": "file",
        "content": "from config import GITHUB_API_URL\r\nfrom crawler.github_api import GitHubAPI\r\nfrom crawler.logger import get_logger\r\nfrom crawler.json_formatter import JSONFormatter\r\nfrom tqdm import tqdm\r\n\r\n\r\nclass GitHubScraper:\r\n    def __init__(self, repo_url):\r\n        self.api = GitHubAPI()\r\n        self.repo_url = repo_url\r\n        self.repo_api_url = self._convert_to_api_url(repo_url)\r\n        self.data_tree = []\r\n        self.logger = get_logger()\r\n\r\n    def _convert_to_api_url(self, repo_url):\r\n        \"\"\"Convert a GitHub repository URL to the API equivalent.\"\"\"\r\n        parts = repo_url.rstrip(\"/\").split(\"/\")\r\n        owner, repo = parts[-2], parts[-1]\r\n        return f\"{GITHUB_API_URL}/repos/{owner}/{repo}/contents\"\r\n\r\n    def scrape(self):\r\n        \"\"\"Start scraping the GitHub repository.\"\"\"\r\n        self.logger.info(f\"Starting scraping for {self.repo_url}\")\r\n        self._scrape_directory(self.repo_api_url, \"\")\r\n        self.logger.info(f\"Scraping completed for {self.repo_url}\")\r\n        return self.data_tree\r\n\r\n    def _scrape_directory(self, api_url, path_prefix):\r\n        \"\"\"Recursively scrape a directory.\"\"\"\r\n        contents = self.api.fetch_directory(api_url)\r\n        for item in contents:\r\n            if item[\"type\"] == \"file\":\r\n                content = self.api.fetch_file_content(item[\"download_url\"])\r\n                self.data_tree.append({\r\n                    \"path\": f\"{path_prefix}/{item['name']}\",\r\n                    \"type\": \"file\",\r\n                    \"content\": content\r\n                })\r\n                self.logger.info(f\"File scraped: {item['path']}\")\r\n            elif item[\"type\"] == \"dir\":\r\n                self.logger.info(f\"Entering directory: {item['path']}\")\r\n                self._scrape_directory(item[\"url\"], f\"{path_prefix}/{item['name']}\")\r\n"
    },
    {
        "path": "/src/main.py",
        "type": "file",
        "content": "import sys\r\nfrom crawler.scraper import GitHubScraper\r\nfrom crawler.json_formatter import JSONFormatter\r\nfrom utils.helpers import is_valid_url\r\nfrom crawler.logger import get_logger\r\n\r\nlogger = get_logger()\r\n\r\ndef main():\r\n    if len(sys.argv) < 2:\r\n        logger.error(\"Usage: python main.py <GitHub repository URL>\")\r\n        sys.exit(1)\r\n\r\n    repo_url = sys.argv[1]\r\n    if not is_valid_url(repo_url):\r\n        logger.error(\"Invalid GitHub URL. Please provide a valid URL.\")\r\n        sys.exit(1)\r\n\r\n    scraper = GitHubScraper(repo_url)\r\n    data = scraper.scrape()\r\n\r\n    output_file = \"repo_data.json\"\r\n    JSONFormatter.save_to_file(data, output_file)\r\n    logger.info(f\"Data saved to {output_file}\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n"
    },
    {
        "path": "/src/utils/__init__.py",
        "type": "file",
        "content": ""
    },
    {
        "path": "/src/utils/__pycache__/__init__.cpython-310.pyc",
        "type": "file",
        "content": "o\r\r\n\u0000\u0000\u0000\u0000��zg\u0000\u0000\u0000\u0000�\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000@\u0000\u0000\u0000s\u0004\u0000\u0000\u0000d\u0000S\u0000)\u0001N�\u0000r\u0001\u0000\u0000\u0000r\u0001\u0000\u0000\u0000r\u0001\u0000\u0000\u0000�-D:\\Proyectos\\gitCrawler\\src\\utils\\__init__.py�\b<module>\u0001\u0000\u0000\u0000s\u0002\u0000\u0000\u0000\u0004\u0000"
    },
    {
        "path": "/src/utils/__pycache__/helpers.cpython-310.pyc",
        "type": "file",
        "content": "o\r\r\n\u0000\u0000\u0000\u0000i�zgt\u0000\u0000\u0000�\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0003\u0000\u0000\u0000@\u0000\u0000\u0000s\u0012\u0000\u0000\u0000d\u0000e\u0000f\u0002d\u0001d\u0002�\u0004Z\u0001d\u0003S\u0000)\u0004�\u0003urlc\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0003\u0000\u0000\u0000C\u0000\u0000\u0000s\n\u0000\u0000\u0000|\u0000�\u0000d\u0001�\u0001S\u0000)\u0002z\u0018Check if a URL is valid.z\u0013https://github.com/)\u0001�\nstartswith)\u0001r\u0001\u0000\u0000\u0000�\u0000r\u0003\u0000\u0000\u0000�,D:\\Proyectos\\gitCrawler\\src\\utils\\helpers.py�\fis_valid_url\u0001\u0000\u0000\u0000s\u0002\u0000\u0000\u0000\n\u0002r\u0005\u0000\u0000\u0000N)\u0002�\u0003strr\u0005\u0000\u0000\u0000r\u0003\u0000\u0000\u0000r\u0003\u0000\u0000\u0000r\u0003\u0000\u0000\u0000r\u0004\u0000\u0000\u0000�\b<module>\u0001\u0000\u0000\u0000s\u0002\u0000\u0000\u0000\u0012\u0000"
    },
    {
        "path": "/src/utils/constants.py",
        "type": "file",
        "content": ""
    },
    {
        "path": "/src/utils/helpers.py",
        "type": "file",
        "content": "def is_valid_url(url : str):\r\n    \"\"\"Check if a URL is valid.\"\"\"\r\n    return url.startswith(\"https://github.com/\")\r\n"
    },
    {
        "path": "/tests/__init__.py",
        "type": "file",
        "content": ""
    },
    {
        "path": "/tests/test_github_api.py",
        "type": "file",
        "content": "import unittest\r\nfrom src.crawler.github_api import GitHubAPI\r\n\r\n\r\nclass TestGitHubAPI(unittest.TestCase):\r\n    def setUp(self):\r\n        self.api = GitHubAPI()\r\n\r\n    def test_fetch_directory(self):\r\n        response = self.api.fetch_directory(\"https://api.github.com/repos/tldraw/tldraw/contents\")\r\n        self.assertIsInstance(response, list)\r\n\r\n    def test_fetch_file_content(self):\r\n        content = self.api.fetch_file_content(\"https://raw.githubusercontent.com/tldraw/tldraw/main/.dockerignore\")\r\n        self.assertIsInstance(content, str)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    unittest.main()\r\n"
    },
    {
        "path": "/tests/test_json_formatter.py",
        "type": "file",
        "content": ""
    },
    {
        "path": "/tests/test_scrapper.py",
        "type": "file",
        "content": ""
    }
]